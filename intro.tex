\chapter{Introduction}

\section{Cryptographic Hash Functions}

A cryptographic hash function, is a function that accepts an arbitrarily long input string, and outputs
a fixed length string that is uniquely related to the input string. The output string is either called message
digest, or hash value of the given input string. Hash functions should be one way, that is given the message
digest, it should not be possible to find the input string. Two input strings even differing in a single
bit should output two different hash values, that are not close or display any co-relation to each other.

\section{The need for cryptographic hash function} 

Digital signatures are method of authenticating electronic documents, and hash functions are used to ease the
process of digitally signing a document. Digital signatures based on asymmetric algorithm like RSA, have a input 
size limitation of around 128 to 324 bytes \cite{00017}. However most documents in practice are longer
than 324 bytes.

One approach would be to divide the message into blocks of size acceptable by that of the signing 
algorithm, and sign each block separately. However, there are disadvantages to this are -

\begin{enumerate}
  \item \emph{Computationally intensive:} Modular exponentiation of large integers used in asymmetric
  algorithms are resource intensive. For signing, multiple blocks of message, the resource utilization
  is pronounced. Additionally, not only the sender but the receiver will also have to do the same resource
  intensive operations.
  \item \emph{Overheads:} The signature is of the same length as the message. This increases the overheads
  in storage and transmission.
  \item \emph{Security concerns:} An attacker could remove, or reorder, or reconstruct new message and 
  signatures from the previous message and signature pairs. Though attacker, cannot manipulate the individual
  blocks, but safety of the entire message is compromised.
\end{enumerate}

Thus to eliminate the overheads, and security limitations; a method is required to uniquely generate fixed
size finger print of arbitrarily large message blocks. Hash functions, fill this void of signing large messages.

\section{Standards and NIST Competition} 

  \subsection{Secure Hashing Algorithm SHA-0 and SHA-1}

  In 1993 National Security Agency(NSA) proposed SHA-0 as standard hashing algorithm, which was later
  standardised by NIST. However by 1995, Florent Chabaud and Antoine Joux, found collisions in SHA-0 
  with complexity of $2^{61}$. In 2004, Eli Biham and Chen found near collisions for SHA-0, about 142 
  out of 160 bits to be equal. Full  collisions were also found, when the number of rounds for the 
  algorithm were reduced from 80 to 62.

  In 1995 SHA-0 was replaced by SHA-1 designed by NSA \cite{00006, 00007}. It has block size of 512 bits
  and output of 160 bits, which is similar to that of SHA-0. SHA-1 has an additional circular shift operation, 
  that rectifies the weakness in SHA-0.

  In 2005 a team from Shandong University in China consisting of Xiaoyun Wang, Yiqun Lisa Yin, 
  and Hongbo Yu, found a way to find collisions on full version of SHA-1 requiring $2^{69}$ operations. 
  This is less than the number of operations required by brute force search for finding colliisions, which
  is $2^{80}$ \cite{00010}. An ideal hash function requires the number of operations to find a collision 
  to be equal to a brute force search, to be considered secure. 

  In October 2012, Jesse Walker from Skein team estimated the computational cost for finding collisions 
  in SHA-1 to be \$ 2.77 million, based on HashClash developed by Marc Stevens \cite{00008}.

  \begin{table}[h]
    \begin{center}
    \begin{tabular}{ *{5}{c} }
      \hline
      Algorithm & Message Size & Block Size & Word Size & Hash Value Size \\ \hline \hline
      SHA-1   & \textless $2^{64}$  bits & 512  bits & 32 bits & 160 bits \\   
      SHA-224 & \textless $2^{64}$  bits & 512  bits & 32 bits & 224 bits \\   
      SHA-256 & \textless $2^{64}$  bits & 512  bits & 32 bits & 256 bits \\   
      SHA-384 & \textless $2^{128}$ bits & 1024 bits & 64 bits & 384 bits \\   
      SHA-512 & \textless $2^{128}$ bits & 1024 bits & 64 bits & 512 bits \\
      \hline
    \end{tabular}
    \caption{ Secure Hash Algorithms as specified in FIPS 180-2} 
  \end{center}
  \end{table}

  \subsection{SHA-2}

  SHA-2 was designed by NSA, and released in 2001 by NIST. It is a family of hash functions consisting of 
  SHA-224, SHA-256, SHA-384, SHA-512. Table 1.1 gives a brief overview of specifications
  of SHA-1 and family of SHA-2 hash functions. The number suffix after the SHA acronym, 
  indicates the bit length, of the output of that hash function. Although SHA-2 family of algorithms
  were influenced by SHA-1 design, but the attacks on SHA-1 have not been successfully extended completely
  to SHA-2.

  Collisions were found on SHA-256 and SHA-512, that were reduced to 22 permutation rounds. Computational
  operations to find collisions in 23 and 24 reduced rounds of SHA-256 were $2^{11.5}$ and $2^{28.5}$ 
  corresponding calls to their respectively reduced functions. For SHA-512 reduced versions of 23 and 24 rounds, 
  the corresponding values for were $2^{16.5}$ and $2^{32.5}$ calls \cite{00012}.
  Since, SHA-2 family relies on the Merkle-Damg\r{a}rd construction, the whole
  process of creation of hash can be considered as repeated application of certain operations generally called
  as compression function, on the input cumulatively. The rounds here refer to the number of times the permutations
  applied to the input during its compression.

  Preimage attacks have been a success, on 41 reduced rounds of SHA-256 and 46 reduced rounds of SHA-512. 
  As per specifications, SHA-256 contains 64 rounds, while SHA-512 consists of 80 rounds \cite{00011}. It can be
  said that SHA-2 functions are partially susceptible to preimage attacks.

  \subsection{NIST competition and SHA-3}

  In November 2, 2007 NIST through a Federal Register Notice announced a public competition for selection of
  new SHA-3 algorithm. This was done in response to advances madei in cryptanalysis of SHA-2.
  Submission requirements stated to provide a cover sheet, algorithm specifications and supporting
  documentation, optimized implementations as per specifications of NIST, and intellectual property statements.

  Submissions for the competition were accepted till October 31, 2008, and 51 candidates from 64 submissions
  for first round of competition were announced on December 9, 2008. On October 2, 2012 NIST announced the 
  winner of the competition to be Keccak, amongst the other four finalist, which were BLAKE, Gr{\o}stl, JH
  and Skein. Keccak was chosen for its' large security margin, efficient hardware implementation, and 
  flexibility.
  
\section{Our work}

In this project we compare the reduced versions of three hashing algorithms, that made it to final round in SHA-3.
They are BLAKE, Gr{\o}stl, and Keccak. The three algorithms are first compared on the basis of reduced number of
permutation rounds, that are kept equal for all three of them. After that we compare variants of Keccak hashing
algorithm, that have their internal state reduced by different amounts. For this part also we reduce the number
of permutation rounds.

The comparison is done on the basis of near collisions found in the reduced version of the algorithms. A
$\epsilon / n $ bit near collision for hash function h and two messages $M_{1}$ and $M_{2}$, where 
$M_{1} \neq M_{2}$ is defined as
\begin{center}$ HW( h( M_{1}, CV ) \oplus h( M_{2}, CV ) ) = n - \epsilon $\end{center}
where HW is the Hamming weight, and CV is the chaining value, and n is the hash size in bits. 60 different message
pairs are made from a seed string "A quick brown fox jumps over the lazy dog", and 60 other strings that differ
from seed string in one bit. The output of the message pairs is compared for a near collision, and improvements
on it are made by iterating it over search algorithms that are: hill climbing, simulated annealing, tabu search 
and random selection.

\begin{center}
  \framebox
  {
    \parbox{400pt}
    {
      \centering \textsc{Hypothesis} \\
      \begin{itemize}
      \item Reduced state Keccak, has better resistance to near collisions than BLAKE and Gr{\o}stl. For the
      attack algorithms hill climbing, simulated annealing, tabu search and random selection.
      \item Simulated annealing and tabu search, are better at finding near collisions compared to hill 
      climbing and random selection.
      \item State size has no effect on efficiency of Keccak permutation rounds.
      \end{itemize}
    }
  }
\end{center}

Through our experiments as formulated through our hypothesis, we aim to determine a search algorithm, that
would be optimum in finding near collisions in reduced versions of hashing algorithm. In our comparison of
the three SHA-3 finalist algorithm, we try to evaluate the winner Keccak and its competitors efficiency, when
the number of permuation rounds are reduced. Lastly we investigate if reduced internal state size of Keccak,
in any way reduces effectiveness of permutation rounds, if the permutation rounds are reduced.
